<div align="center">

English | [ç®€ä½“ä¸­æ–‡](./README-zh_CN.md)


<h1>DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception</h1>

Official PyTorch implementation of [DocLayout-YOLO](https://arxiv.org/abs/2410.12628).

[![arXiv](https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg)](https://arxiv.org/abs/2410.12628) [![Online Demo](https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-yellow)](https://huggingface.co/spaces/opendatalab/DocLayout-YOLO) [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97-Models%20and%20Data-yellow)](https://huggingface.co/collections/juliozhao/doclayout-yolo-670cdec674913d9a6f77b542)

</div>
    
## Abstract

> We present DocLayout-YOLO, a real-time and robust layout detection model for diverse documents, based on YOLO-v10. This model is enriched with diversified document pre-training and structural optimization tailored for layout detection. In the pre-training phase, we introduce Mesh-candidate BestFit, viewing document synthesis as a two-dimensional bin packing problem, and create a large-scale diverse synthetic document dataset, DocSynth-300K. In terms of model structural optimization, we propose a module with Global-to-Local Controllability for precise detection of document elements across varying scales. 


<p align="center">
  <img src="assets/comp.png" width=52%>
  <img src="assets/radar.png" width=44%> <br>
</p>

## News ğŸš€ğŸš€ğŸš€

**2024.10.25** ğŸ‰ğŸ‰  **Mesh-candidate Bestfit** code is released. Mesh-candidate Bestfit is an automatic pipeline which can synthesize large-scale, high-quality, and visually appealing document layout detection dataset. Tutorial and example data are available in [here](./mesh-candidate_bestfit).

**2024.10.23** ğŸ‰ğŸ‰  **DocSynth300K dataset** is released on [ğŸ¤—Huggingface](https://huggingface.co/datasets/juliozhao/DocSynth300K), DocSynth300K is a large-scale and diverse document layout analysis pre-training dataset, which can largely boost model performance.

**2024.10.21** ğŸ‰ğŸ‰  **Online demo** available on [ğŸ¤—Huggingface](https://huggingface.co/spaces/opendatalab/DocLayout-YOLO).

**2024.10.18** ğŸ‰ğŸ‰  DocLayout-YOLO is implemented in **[PDF-Extract-Kit](https://github.com/opendatalab/PDF-Extract-Kit)** for document context extraction.

**2024.10.16** ğŸ‰ğŸ‰  **Paper** now available on [ArXiv](https://arxiv.org/abs/2410.12628).   


## Quick Start

[Online Demo](https://huggingface.co/spaces/opendatalab/DocLayout-YOLO) is now available. For local development, follow steps below:

### 1. Environment Setup

Follow these steps to set up your environment:

```bash
conda create -n doclayout_yolo python=3.10
conda activate doclayout_yolo
pip install -e .
```

**Note:** If you only need the package for inference, you can simply install it via pip:

```bash
pip install doclayout-yolo
```

### 2. Prediction

You can make predictions using either a script or the SDK:

- **Script**

  Run the following command to make a prediction using the script:

  ```bash
  python demo.py --model path/to/model --image-path path/to/image
  ```

- **SDK**

  Here is an example of how to use the SDK for prediction:

  ```python
  import cv2
  from doclayout_yolo import YOLOv10

  # Load the pre-trained model
  model = YOLOv10("path/to/provided/model")

  # Perform prediction
  det_res = model.predict(
      "path/to/image",   # Image to predict
      imgsz=1024,        # Prediction image size
      conf=0.2,          # Confidence threshold
      device="cuda:0"    # Device to use (e.g., 'cuda:0' or 'cpu')
  )

  # Annotate and save the result
  annotated_frame = det_res[0].plot(pil=True, line_width=5, font_size=20)
  cv2.imwrite("result.jpg", annotated_frame)
  ```


We provide model fine-tuned on **DocStructBench** for prediction, **which is capable of handing various document types**. Model can be downloaded from [here](https://huggingface.co/juliozhao/DocLayout-YOLO-DocStructBench/tree/main) and example images can be found under ```assets/example```.

<p align="center">
  <img src="assets/showcase.png" width=100%> <br>
</p>


**Note:** For PDF content extraction, please refer to [PDF-Extract-Kit](https://github.com/opendatalab/PDF-Extract-Kit/tree/main) and [MinerU](https://github.com/opendatalab/MinerU).

**Note:** Thanks to [NielsRogge](https://github.com/NielsRogge), DocLayout-YOLO now supports implementation directly from ğŸ¤—Huggingface, you can load model as follows:

```python
filepath = hf_hub_download(repo_id="juliozhao/DocLayout-YOLO-DocStructBench", filename="doclayout_yolo_docstructbench_imgsz1024.pt")
model = YOLOv10(filepath)
```

or directly load using ```from_pretrained```:

```python
model = YOLOv10.from_pretrained("juliozhao/DocLayout-YOLO-DocStructBench")
```

more details can be found at [this PR](https://github.com/opendatalab/DocLayout-YOLO/pull/6).

**Note:** Thanks to [luciaganlulu](https://github.com/luciaganlulu), DocLayout-YOLO can perform batch inference and prediction. Instead of passing single image into ```model.predict``` in ```demo.py```, pass a **list of image path**. Besides, due to batch inference is not implemented before ```YOLOv11```, you should manually change ```batch_size``` in [here](doclayout_yolo/engine/model.py#L431).

## DocSynth300K Dataset

<p align="center">
  <img src="assets/docsynth300k.png" width=100%>
</p>

### Data Download

Use following command to download dataset(about 113G):

```python
from huggingface_hub import snapshot_download
# Download DocSynth300K
snapshot_download(repo_id="juliozhao/DocSynth300K", local_dir="./docsynth300k-hf", repo_type="dataset")
# If the download was disrupted and the file is not complete, you can resume the download
snapshot_download(repo_id="juliozhao/DocSynth300K", local_dir="./docsynth300k-hf", repo_type="dataset", resume_download=True)
```

### Data Formatting & Pre-training

If you want to perform DocSynth300K pretraining, using ```format_docsynth300k.py``` to convert original ```.parquet``` format into ```YOLO``` format. The converted data will be stored at ```./layout_data/docsynth300k```.

```bash
python format_docsynth300k.py
```

To perform DocSynth300K pre-training, use this [command](assets/script.sh#L2). We default use 8GPUs to perform pretraining. To reach optimal performance, you can adjust hyper-parameters such as ```imgsz```, ```lr``` according to your downstream fine-tuning data distribution or setting.

**Note:** Due to memory leakage in YOLO original data loading code, the pretraining on large-scale dataset may be interrupted unexpectedly, use ```--pretrain last_checkpoint.pt --resume``` to resume the pretraining process.

## Training and Evaluation on Public DLA Datasets

### Data Preparation

1. specify  the data root path

Find your ultralytics config file (for Linux user in ```$HOME/.config/Ultralytics/settings.yaml)``` and change ```datasets_dir``` to project root path.

2. Download prepared yolo-format D4LA and DocLayNet data from below and put to ```./layout_data```:

| Dataset | Download |
|:--:|:--:|
| D4LA | [link](https://huggingface.co/datasets/juliozhao/doclayout-yolo-D4LA) |
| DocLayNet | [link](https://huggingface.co/datasets/juliozhao/doclayout-yolo-DocLayNet) |

the file structure is as follows:

```bash
./layout_data
â”œâ”€â”€ D4LA
â”‚Â Â  â”œâ”€â”€ images
â”‚Â Â  â”œâ”€â”€ labels
â”‚Â Â  â”œâ”€â”€ test.txt
â”‚Â Â  â””â”€â”€ train.txt
â””â”€â”€ doclaynet
    â”œâ”€â”€ images
 Â Â  â”œâ”€â”€ labels
 Â Â  â”œâ”€â”€ val.txt
 Â Â  â””â”€â”€ train.txt
```

### Training and Evaluation

Training is conducted on 8 GPUs with a global batch size of 64 (8 images per device). The detailed settings and checkpoints are as follows:

| Dataset | Model | DocSynth300K Pretrained? | imgsz | Learning rate | Finetune | Evaluation | AP50 | mAP | Checkpoint |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| D4LA | DocLayout-YOLO | &cross; | 1600 | 0.04 | [command](assets/script.sh#L5) | [command](assets/script.sh#L11) | 81.7 | 69.8 | [checkpoint](https://huggingface.co/juliozhao/DocLayout-YOLO-D4LA-from_scratch) |
| D4LA | DocLayout-YOLO | &check; | 1600 | 0.04 | [command](assets/script.sh#L8) | [command](assets/script.sh#L11) | 82.4 | 70.3 | [checkpoint](https://huggingface.co/juliozhao/DocLayout-YOLO-D4LA-Docsynth300K_pretrained) |
| DocLayNet | DocLayout-YOLO | &cross; | 1120 | 0.02 | [command](assets/script.sh#L14) | [command](assets/script.sh#L20) | 93.0 | 77.7 | [checkpoint](https://huggingface.co/juliozhao/DocLayout-YOLO-DocLayNet-from_scratch) |
| DocLayNet | DocLayout-YOLO | &check; | 1120 | 0.02 | [command](assets/script.sh#L17) | [command](assets/script.sh#L20) | 93.4 | 79.7 | [checkpoint](https://huggingface.co/juliozhao/DocLayout-YOLO-DocLayNet-Docsynth300K_pretrained) |

The DocSynth300K pretrained model can be downloaded from [here](https://huggingface.co/juliozhao/DocLayout-YOLO-DocSynth300K-pretrain). Change ```checkpoint.pt``` to the path of model to be evaluated during evaluation.


## í•œêµ­ ì‹œí—˜ì§€ ë ˆì´ì•„ì›ƒ ê°€ì´ë“œ (K-Exam)

ë³¸ ì„¹ì…˜ì€ í•œêµ­ì–´ ì‹œí—˜ì§€(ì¤‘/ê³ êµ ì§€í•„í‰ê°€ ë“±)ì— íŠ¹í™”ëœ ìµœì†Œ ë¼ë²¨ ìŠ¤í‚¤ë§ˆ, ë°ì´í„° ì¤€ë¹„, ì¦ê°•ê³¼ ì „ì´í•™ìŠµ, í‰ê°€Â·í›„ì²˜ë¦¬, ì†ŒëŸ‰ ë°ì´í„° ì „ëµê¹Œì§€ í•œ ë²ˆì— ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ì •ë¦¬ëœ ê°€ì´ë“œì…ë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œ ë¹ ë¥´ê²Œ ëŒë¦´ ìˆ˜ ìˆë„ë¡ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸/ì˜ˆì‹œë¥¼ ëª¨ë‘ ì œê³µí•©ë‹ˆë‹¤.

### 1) í•œêµ­ ì‹œí—˜ì§€ìš© ë¼ë²¨ ìŠ¤í‚¤ë§ˆ ì •ì˜

ì‹œí—˜ì§€ì— íŠ¹í™”ëœ ìµœì†ŒÂ·í•„ìˆ˜ ë¸”ë¡ ìœ„ì£¼ë¡œ ì‹œì‘í•˜ê³ , ì„¸ë¶€ ë¶„ë¦¬ëŠ” í›„ì²˜ë¦¬ ê·œì¹™(ë¼ì¸/ë²ˆí˜¸ íƒì§€)ë¡œ í•´ê²°í•˜ëŠ” ê²ƒì´ ì•ˆì •ì ì…ë‹ˆë‹¤.

- 10ê°œ ê¶Œì¥ í´ë˜ìŠ¤
  1. header (í•™êµ/í•™ê¸°/ê³¼ëª©/ì ìˆ˜ ì˜ì—­)
  2. passage (ê³µí†µ ì§€ë¬¸/ìë£Œ)
  3. question_stem (ë°œë¬¸)
  4. choice_block (ì„ ì§€ ë¬¶ìŒ ì „ì²´)
  5. choice_item (â‘ ~â‘¤ ë“± ê°œë³„ ì„ ì§€)
  6. diagram (ê·¸ë¦¼)
  7. table (í‘œ)
  8. formula_block (ìˆ˜ì‹ ë¸”ë¡)
  9. answer_area (ì„œìˆ /ë‹¨ë‹µ ë‹µì•ˆ ì¹¸)
  10. footer (í˜ì´ì§€ ë²ˆí˜¸/ìœ ì˜ì‚¬í•­)

- ë©”ëª¨
  - ì´ˆíŒì—ì„œëŠ” choice_blockë§Œ ë‘ê³ , ë‚´ë¶€ â‘ ~â‘¤ëŠ” í›„ì²˜ë¦¬(ì› ì•ˆ ìˆ«ì/â€˜ã„±, ã„´â€™ ë“± íŒ¨í„´ ì¸ì‹)ë¡œ ë¶„ë¦¬í•´ë„ ë©ë‹ˆë‹¤. (í´ë˜ìŠ¤ ìˆ˜â†“ â†’ ë¼ë²¨ ì¼ê´€ì„±â†‘)
  - í•„ìš” ì‹œ question_numberë¥¼ ë³„ë„ í´ë˜ìŠ¤ë¡œ ë‘ë©´ ë¬¸í•­ ë‹¨ìœ„ í´ëŸ¬ìŠ¤í„°ë§ì´ ì‰¬ì›Œì§‘ë‹ˆë‹¤(ë²ˆí˜¸â†’ê°€ê¹Œìš´ stem/choice ë§¤ì¹­).

### 2) ë°ì´í„° ì¤€ë¹„(ë¼ë²¨ë§ â†’ YOLO í¬ë§·)

- ë¼ë²¨ë§ ë„êµ¬: Label Studio, CVAT, Roboflow ë“±(ë°•ìŠ¤+í´ë˜ìŠ¤)
- í•´ìƒë„: 300DPI ìŠ¤ìº” ì›ë³¸ ë˜ëŠ” PDFâ†’ì´ë¯¸ì§€(ê¸´ ë³€ 2000~3000px ê¶Œì¥)
- í¬ë§·: YOLO í…ìŠ¤íŠ¸ ë¼ë²¨(í´ë˜ìŠ¤ ID, ì •ê·œí™” bbox: xc yc w h)
- ë””ë ‰í„°ë¦¬ êµ¬ì¡°(ì´ ë¦¬í¬ì˜ D4LA/DocLayNet ì˜ˆì‹œì™€ ë™ì¼í•˜ê²Œ êµ¬ì„± ê¶Œì¥)

```bash
./layout_data/kexam
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ train
â”‚   â””â”€â”€ val
â””â”€â”€ labels
    â”œâ”€â”€ train
    â””â”€â”€ val
```

- data.yaml ì˜ˆì‹œ(íŒŒì¼ ê²½ë¡œ: `layout_data/kexam/kexam.yaml`)

```yaml
path: ./layout_data/kexam
train: images/train
val: images/val
names:
  0: header
  1: passage
  2: question_stem
  3: choice_block
  4: choice_item
  5: diagram
  6: table
  7: formula_block
  8: answer_area
  9: footer
```

Important: ë³¸ ë¦¬í¬ì˜ `train.py`/`val.py`ëŠ” `--data` ì¸ìì— `.yaml` í™•ì¥ìë¥¼ ìë™ìœ¼ë¡œ ë¶™ì…ë‹ˆë‹¤. ì‹¤í–‰ ì‹œì—ëŠ” í™•ì¥ìë¥¼ ì œì™¸í•œ ë² ì´ìŠ¤ ê²½ë¡œë§Œ ë„˜ê¸°ì„¸ìš”. ì˜ˆ: `--data layout_data/kexam/kexam` (O), `--data layout_data/kexam/kexam.yaml` (X)

- PDF â†’ ì´ë¯¸ì§€ ë³€í™˜(ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸)

```bash
python /Users/jiwon/Workspace/DocLayout-YOLO/scripts/pdf_to_images.py \
  "/Users/jiwon/Workspace/DocLayout-YOLO/pdfs" \
  -o "/Users/jiwon/Workspace/DocLayout-YOLO/layout_data/kexam" \
  --fmt png --dpi 300 --max-side 3000 --workers 4 --optimize
```

- Label Studio/CVAT ë‚´ë³´ë‚´ê¸° í›„ train/val ë¶„í• (ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸)

```bash
python /Users/jiwon/Workspace/DocLayout-YOLO/scripts/train-val-splitter.py
```

ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ ë³€ìˆ˜(`export_dir`)ë¥¼ ì‹¤ì œ ë‚´ë³´ë‚´ê¸° ê²½ë¡œë¡œ ë§ì¶˜ ë’¤ ì‹¤í–‰í•˜ì„¸ìš”. ì´ë¯¸ì§€/ë¼ë²¨ íŒŒì¼ëª…ì´ 1:1ë¡œ ë§¤ì¹­ë˜ë„ë¡ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

### 3) ë„ë©”ì¸ ì¦ê°•(í•œêµ­ ì‹œí—˜ì§€ì— ì˜ ë¨¹íˆëŠ” ê²ƒ)

- ë³µì‚¬/ìŠ¤ìº” ì•„í‹°íŒ©íŠ¸: ê°€ìš°ì‹œì•ˆ/ëª¨ì…˜ ë¸”ëŸ¬, JPEG ì••ì¶•, ì„ì˜ ê·¸ë ˆì´ë‹
- íœ˜ì–´ì§/ê·¸ë¦¼ì: ë°ê¸°/ëŒ€ë¹„/ì„€ë„ìš° ê·¸ë¼ë””ì–¸íŠ¸ í•©ì„±
- ì•½ê°„ì˜ ê¸°ìš¸ê¸°/ì»·ì˜¤í”„: Â±1~2.5Â° íšŒì „, í…Œë‘ë¦¬ ì˜ë¦¼
- í‘ë°±/ì €ì±„ë„ ì „í™˜(í•™êµ í”„ë¦°íŠ¸ ëŠë‚Œ)
- í°íŠ¸ ë‹¤ì–‘í™”: í•œê¸€ ë³¸ë¬¸/ë²ˆí˜¸(â‘ â‘¡â€¦/ã„±ã„´ã„·) í•©ì„±(ì„ ì§€ ê²€ì¶œ ê°•ê±´ì„±â†‘)
- ìˆ˜ì‹/í‘œ ë°€ì§‘ í•©ì„±(ìˆ˜í•™/ê³¼í•™ ê³¼ëª© ëŒ€ì‘)

DocLayout-YOLOëŠ” í•©ì„± ë°ì´í„° ì‚¬ì „í•™ìŠµ(â€œMesh-candidate BestFitâ€ ê¸°ë°˜ DocSynth-300K)ì„ ê°•ì¡°í•©ë‹ˆë‹¤. í•œêµ­ ì‹œí—˜ì§€ í…œí”Œë¦¿ì„ í‰ë‚´ ë‚¸ í•©ì„± ë¯¸ë‹ˆì…‹ì„ ê³ë“¤ì´ë©´ ì†ŒëŸ‰ ì‹¤ë°ì´í„°ë¡œë„ ì„±ëŠ¥ì´ ë¹ ë¥´ê²Œ ë¶™ìŠµë‹ˆë‹¤.

### 4) í•™ìŠµ ì‹œì‘(ì „ì´í•™ìŠµ)

- í™˜ê²½/ì¶”ë¡  ì„¤ì¹˜(ì„ íƒ): ìƒë‹¨ Quick Start ì°¸ê³  ë˜ëŠ” ì•„ë˜ ê°„ë‹¨ ì„¤ì¹˜

```bash
conda create -n doclayout_yolo python=3.10 -y
conda activate doclayout_yolo
pip install -e /Users/jiwon/Workspace/DocLayout-YOLO
```

- ì œê³µ ì²´í¬í¬ì¸íŠ¸(ì˜ˆ: DocStructBench) ê²½ë¡œ ì˜ˆì‹œ: `/Users/jiwon/Workspace/DocLayout-YOLO/models/YOLO/doclayout_yolo_docstructbench_imgsz1280_2501.pt`

- ë‹¨ì¼/ì†Œìˆ˜ GPU ì „ì´í•™ìŠµ ì‹¤í–‰ ì˜ˆì‹œ(ì´ ë¦¬í¬ì˜ `train.py` ì¸í„°í˜ì´ìŠ¤ ê¸°ì¤€)

```bash
# ì‚¬ì „í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ì „ì´í•™ìŠµ
python /Users/jiwon/Workspace/DocLayout-YOLO/train.py \
  --data layout_data/kexam/kexam \
  --model m \
  --epoch 80 \
  --batch-size 8 \
  --image-size 1280 \
  --lr0 0.01 \
  --device 0 \
  --workers 4 \
  --val 1 \
  --val-period 1 \
  --project /Users/jiwon/Workspace/DocLayout-YOLO/dataset/kexam \
  --pretrain /Users/jiwon/Workspace/DocLayout-YOLO/models/YOLO/doclayout_yolo_docstructbench_imgsz1280_2501.pt

# ìŠ¤í¬ë˜ì¹˜(ì‚¬ì „í•™ìŠµ ì—†ì´, ëª¨ë¸ êµ¬ì¡°: yolov10m)
python /Users/jiwon/Workspace/DocLayout-YOLO/train.py \
  --data layout_data/kexam/kexam \
  --model m \
  --epoch 80 \
  --batch-size 8 \
  --image-size 1600 \
  --lr0 0.02 \
  --device 0 \
  --workers 4 \
  --val 1 \
  --val-period 1 \
  --project /Users/jiwon/Workspace/DocLayout-YOLO/dataset/kexam
```

Hints

- í•œêµ­ ì‹œí—˜ì§€ëŠ” ì†Œë¬¼ì²´(ë²ˆí˜¸/ì›í˜•ìˆ«ì/ìˆ˜ì‹)ê°€ ë§ì•„ `--image-size 1280~1600`ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
- ì—¬ëŸ¬ GPU ì‚¬ìš© ì‹œ `--device 0,1,2,...` í˜•íƒœë¡œ ì§€ì •í•˜ê³  ì „ì—­ ë°°ì¹˜ë¥¼ ëŠ˜ë¦¬ë©´ ìˆ˜ë ´ì´ ì•ˆì •ì ì…ë‹ˆë‹¤.

### 5) í‰ê°€/í›„ì²˜ë¦¬(ë¬¸í•­ ë‹¨ìœ„ë¡œ ë¬¶ê¸°)

- í‰ê°€(mAP@50/50:95 + í´ë˜ìŠ¤ë³„ F1), íŠ¹íˆ ë¦¬ì½œ í™•ì¸ ê¶Œì¥. ì´ ë¦¬í¬ì˜ `val.py`ë¡œ ë°”ë¡œ í‰ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

```bash
python /Users/jiwon/Workspace/DocLayout-YOLO/val.py \
  --data layout_data/kexam/kexam \
  --model /Users/jiwon/Workspace/DocLayout-YOLO/dataset/kexam/<ì‹¤í–‰_í´ë”>/weights/best.pt \
  --batch-size 8 \
  --device 0
```

- ë¬¸í•­ ì¬êµ¬ì„±(ë ˆì´ì•„ì›ƒ â†’ ë¬¸í•­ ë‹¨ìœ„) ê·œì¹™ ì˜ˆì‹œ
  1. question_number(ë˜ëŠ” stem bbox) ì¤‘ì‹¬ìœ¼ë¡œ ê°€ê¹Œìš´ choice_block/diagram/table ë§¤ì¹­(ê±°ë¦¬/ì •ë ¬)
  2. ë‹¤ë‹¨(2-column) ì‹œí—˜ì§€ëŠ” ì„¸ë¡œ ê³µë°± í”„ë¡œíŒŒì¼ë¡œ ì—´ ë¶„í•  í›„, ê° ì—´ ë‚´ë¶€ì—ì„œ y-ì¢Œí‘œ ì •ë ¬
  3. choice_blockë§Œ ê²€ì¶œí–ˆë‹¤ë©´ ë‚´ë¶€ë¥¼ OCR/íŒ¨í„´ìœ¼ë¡œ â€˜â‘ â‘¡â€¦â€™ ë˜ëŠ” â€˜ã„±ã„´ã„·â€™ ìŠ¬ë¼ì´ìŠ¤
  4. NMSëŠ” í´ë˜ìŠ¤ë³„ë¡œ ìˆ˜í–‰, choice_itemì€ IoUë¥¼ ë‚®ì¶° ê³¼ì‰ ì–µì œ ë°©ì§€

- ê°„ë‹¨í•œ ë¬¸í•­ ê·¸ë£¹í•‘ íŒŒì´ì¬ ì˜ˆì‹œ(ê°œë…ìš©)

```python
from typing import List, Tuple

# bbox: (xc, yc, w, h, score, cls)
Box = Tuple[float, float, float, float, float, int]

def iou_xywh(a: Box, b: Box) -> float:
    ax, ay, aw, ah = a[0] - a[2]/2, a[1] - a[3]/2, a[2], a[3]
    bx, by, bw, bh = b[0] - b[2]/2, b[1] - b[3]/2, b[2], b[3]
    inter_x1, inter_y1 = max(ax, bx), max(ay, by)
    inter_x2, inter_y2 = min(ax+aw, bx+bw), min(ay+ah, by+bh)
    inter = max(0.0, inter_x2 - inter_x1) * max(0.0, inter_y2 - inter_y1)
    union = aw*ah + bw*bh - inter + 1e-6
    return inter / union

def group_by_question(stems: List[Box], others: List[Box], x_weight: float = 0.5) -> List[dict]:
    groups = []
    stems_sorted = sorted(stems, key=lambda b: (b[1], b[0]))
    for s in stems_sorted:
        sx, sy = s[0], s[1]
        bucket = {"stem": s, "attached": []}
        for o in others:
            ox, oy = o[0], o[1]
            # ê±°ë¦¬ ê°€ì¤‘(ì—´ ì •ë ¬ ìš°ì„ ) + y ìš°ì„  ì •ë ¬
            d = abs(oy - sy) + x_weight * abs(ox - sx)
            # stem í•˜ë‹¨ ê·¼ì²˜/ìš°ì¸¡ ê·¼ì²˜ ìš°ì„ (ê°„ë‹¨í•œ ì¡°ê±´)
            if oy >= sy - 0.05:
                bucket["attached"].append((d, o))
        bucket["attached"].sort(key=lambda t: (t[0], -t[1][4]))  # ê±°ë¦¬â†’score ìˆœ
        groups.append(bucket)
    return groups
```

### 6) ë°ì´í„°ê°€ ì ì„ ë•Œì˜ íŒ

- ë„ë©”ì¸ í•©ì„±: í•™êµëª…/í•™ê¸°/ë°°ì /ë²ˆí˜¸/ì›í˜•ìˆ«ì/ìˆ˜ì‹/í‘œ ë“±ì„ í¬í•¨í•œ í•©ì„± ì‹œí—˜ì§€ ìˆ˜ë°±~ìˆ˜ì²œ ì¥ ìƒì„± â†’ ì‹¤ë°ì´í„° 50~200í˜ì´ì§€ì™€ í˜¼í•© ë¯¸ì„¸ì¡°ì •
- í”„ë¦¬ì§•: ë°±ë³¸ ì¼ë¶€ ê³ ì •, í—¤ë“œë§Œ ë¯¸ì„¸ì¡°ì •(ê³¼ì í•© ë°©ì§€)
- íƒ€ ë°ì´í„° ì›Œë°ì—…: DocLayNet ì¼ë¶€ í´ë˜ìŠ¤ë¥¼ í•œêµ­ ì‹œí—˜ì§€ ë¼ë²¨ì…‹ì— ë§µí•‘í•´ ë¨¼ì € í•™ìŠµ â†’ ì´í›„ í•œêµ­ ì‹œí—˜ì§€ ì†ŒëŸ‰ íŒŒì¸íŠœë‹. DocLayNetì€ COCO í¬ë§·ìœ¼ë¡œ 8ë§Œ+ í˜ì´ì§€ ì œê³µ

### 7) ìì£¼ ë§‰íˆëŠ” í¬ì¸íŠ¸

- ì´ë¯¸ì§€ í¬ê¸°(imgsz): 1120~1600 ì‚¬ìš© ì˜ˆê°€ ë§ê³ , 32ì˜ ë°°ìˆ˜ë¡œ ë§ì¶”ëŠ” ê´€í–‰. ë„ˆë¬´ ì‘ìœ¼ë©´ ë²ˆí˜¸/ìˆ˜ì‹ ì†Œì‹¤
- í´ë˜ìŠ¤ ê³¼ë„ ë¶„í• : ì´ˆê¸°ì—ëŠ” stem/choice_block/diagram/table/passage ìœ„ì£¼ë¡œ ì‹œì‘ í›„ í•„ìš” ì‹œ í™•ì¥
- íŠ¹ìˆ˜ê¸°í˜¸: ì› ì•ˆ ìˆ«ìÂ·ã„±ã„´ã„·Â·ìˆ˜ì‹ì€ ë‚œì´ë„â†‘ â†’ ì¦ê°•/í•©ì„±ìœ¼ë¡œ ì¶©ë¶„íˆ ë…¸ì¶œ

### 8) ìµœì†Œ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

1. ë¼ë²¨ì…‹(6~10ì¢…) ê²°ì • â†’ 100~300í˜ì´ì§€ ë¼ë²¨ë§
2. YOLO í¬ë§·ìœ¼ë¡œ ì •ë¦¬(`layout_data/kexam/{images,labels}/{train,val}`)
3. ê³µê°œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í›„ `--image-size 1280~1600`, ê°€ëŠ¥í•œ ë°°ì¹˜ë¡œ 50~80 epoch íŒŒì¸íŠœë‹
4. `val.py`ë¡œ í´ë˜ìŠ¤ë³„ ë¦¬ì½œ í™•ì¸ â†’ ëˆ„ë½ ë§ì€ í´ë˜ìŠ¤ ìœ„ì£¼ë¡œ ì¦ê°•/ë¼ë²¨ ë³´ê°•
5. í›„ì²˜ë¦¬ ê·œì¹™ìœ¼ë¡œ ë¬¸í•­ ë‹¨ìœ„ ê·¸ë£¹í•‘(ë²ˆí˜¸â†”ë°œë¬¸â†”ì„ ì§€ ë§¤ì¹­)


## Acknowledgement

The code base is built with [ultralytics](https://github.com/ultralytics/ultralytics) and [YOLO-v10](https://github.com/lyuwenyu/RT-DETR).

Thanks for their great work!

## Star History

If you find our project useful, please add a "star" to the repo. It's exciting to us when we see your interest, which keep us motivated to continue investing in the project!

<picture>
  <source
    media="(prefers-color-scheme: dark)"
    srcset="
      https://api.star-history.com/svg?repos=opendatalab/DocLayout-YOLO&type=Date&theme=dark
    "
  />
  <source
    media="(prefers-color-scheme: light)"
    srcset="
      https://api.star-history.com/svg?repos=opendatalab/DocLayout-YOLO&type=Date
    "
  />
  <img
    alt="Star History Chart"
    src="https://api.star-history.com/svg?repos=opendatalab/DocLayout-YOLO&type=Date"
  />
</picture>

## Citation

```bibtex
@misc{zhao2024doclayoutyoloenhancingdocumentlayout,
      title={DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception}, 
      author={Zhiyuan Zhao and Hengrui Kang and Bin Wang and Conghui He},
      year={2024},
      eprint={2410.12628},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.12628}, 
}

@article{wang2024mineru,
  title={MinerU: An Open-Source Solution for Precise Document Content Extraction},
  author={Wang, Bin and Xu, Chao and Zhao, Xiaomeng and Ouyang, Linke and Wu, Fan and Zhao, Zhiyuan and Xu, Rui and Liu, Kaiwen and Qu, Yuan and Shang, Fukai and others},
  journal={arXiv preprint arXiv:2409.18839},
  year={2024}
}

```
